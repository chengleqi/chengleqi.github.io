[{"categories":["Chaos Engineering"],"content":"真实的就是混沌的. ","date":"2022-05-04","objectID":"/posts/22-05-03/:0:0","tags":["Chaos Mesh","Litmus"],"title":"有意思的混沌工程","uri":"/posts/22-05-03/"},{"categories":["Ingress"],"content":"自动检测和热更新！ ","date":"2022-05-04","objectID":"/posts/22-05-04/:0:0","tags":["Traefik","Cert-Manager"],"title":"云原生代理Traefik","uri":"/posts/22-05-04/"},{"categories":["IaC"],"content":"在k8s中管理你的基础设施😋🍦! 《What is Infrastructure as Code (IaC)?》by RedHat Infrastructure as Code (IaC) is the managing and provisioning of infrastructure through code instead of through manual processes. With IaC, configuration files are created that contain your infrastructure specifications, which makes it easier to edit and distribute configurations. It also ensures that you provision the same environment every time. By codifying and documenting your configuration specifications, IaC aids configuration management and helps you to avoid undocumented, ad-hoc configuration changes. Version control is an important part of IaC, and your configuration files should be under source control just like any other software source code file. Deploying your infrastructure as code also means that you can divide your infrastructure into modular components that can then be combined in different ways through automation. Automating infrastructure provisioning with IaC means that developers don’t need to manually provision and manage servers, operating systems, storage, and other infrastructure components each time they develop or deploy an application. Codifying your infrastructure gives you a template to follow for provisioning, and although this can still be accomplished manually, an automation tool, such as Red Hat® Ansible Automation® Platform, can do it for you. ","date":"2022-05-02","objectID":"/posts/22-05-02/:0:0","tags":["Terraform","Crossplane"],"title":"IaC: 从Terraform到Crossplane","uri":"/posts/22-05-02/"},{"categories":["Kubernetes"],"content":"星际尺度？ ","date":"2022-04-18","objectID":"/posts/22-04-18/:0:0","tags":["Cluster Federation"],"title":"集群联邦和Karmada","uri":"/posts/22-04-18/"},{"categories":["Kubernetes"],"content":"狐狸、小狗和星际章鱼？ ","date":"2022-04-11","objectID":"/posts/22-04-12/:0:0","tags":["CI/CD"],"title":"k8s原生CI/CD","uri":"/posts/22-04-12/"},{"categories":["Serverless"],"content":"服务端的终极？ ","date":"2022-04-11","objectID":"/posts/22-04-11/:0:0","tags":["Knative"],"title":"Knative简记","uri":"/posts/22-04-11/"},{"categories":["Kubernetes"],"content":"Kubernetes the hard way 🥲 why is k8s so fucking complex?\" why is k8s so fucking complex? ","date":"2022-04-05","objectID":"/posts/22-04-05/:0:0","tags":["Golang"],"title":"k8s中的证书机制和远程调试","uri":"/posts/22-04-05/"},{"categories":["Kubernetes"],"content":"一切都是为了通！ ——阅读《深入剖析Kubernetes》第7章有感 为了聚合算力，为了屏蔽基础设施的差异，要让Pod相互连通是重大且现实的问题。 k8s为了解决Pod间的网络连通设置了CNI接口来供各方实现，这样就可以解耦与某种特定网络方案之间的绑定，类似的设计思想在k8s上还体现在CRI和CSI接口的设置上。现在已经有许多实现了CNI接口的成熟的网络方案，比较知名的有flannel、Calico和Cilium。 ","date":"2022-04-01","objectID":"/posts/22-04-01/:0:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"单机容器互联 Docker容器网络有四种模式，host、none、bridge和container，在启动容器的时候可以指定以哪种方式设置网络，不指定的话默认是桥接。 在默认桥接模式下，每个容器都在一个Network Namespace中并且有一张veth网卡插在docker0虚拟交换机上，容器之间的相互通信是通过交换机广播ARP在二层实现的，容器和宿主机之间的通信是通过设置容器的路由表找到插在宿主机上的与docker0网桥同名的docker0网卡实现的。网络流量可以通过tcpdump指定网卡抓包查看，网桥设备也可以通过brctl查看，并且可以查看插在虚拟网桥上的veth pair设备。 桥接模式\" 桥接模式 这里有个小技巧，如果我们想查看k8s集群内DNS请求的情况，但是DNS容器往往不具备bash，所以无法通过docker exec的方式进入容器内抓包，这时可以进入该容器的Network Namespace，再抓包。 // 1、找到CONTAINER_ID，并打印它的NS_ID docker inspect --format \"{{.State.Pid}}\" $CONTAINER_ID // 2、进入该容器的Network Namespace nsenter -n -t $NS_ID // 3、抓DNS包 tcpdump -i eth0 udp dst port 53 ","date":"2022-04-01","objectID":"/posts/22-04-01/:1:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"容器跨主互联 ","date":"2022-04-01","objectID":"/posts/22-04-01/:2:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"flannel k8s经典的网络解决方案是flannel。 flannel backends Flannel may be paired with several different backends. Once set, the backend should not be changed at runtime. VXLAN is the recommended choice. host-gw is recommended for more experienced users who want the performance improvement and whose infrastructure support it (typically it can’t be used in cloud environments). UDP is suggested for debugging only or for very old kernels that don’t support VXLAN. VXLAN Use in-kernel VXLAN to encapsulate the packets. host-gw Use host-gw to create IP routes to subnets via remote machine IPs. Requires direct layer2 connectivity between hosts running flannel. host-gw provides good performance, with few dependencies, and easy set up. UDP Use UDP only for debugging if your network and kernel prevent you from using VXLAN or host-gw. flannel有三种后端实现，UDP、VXLAN和host-gw，其中UDP模式已经废弃，只在调试或者网络和内核不支持使用host-gw和VXLAN模式时可以选择使用。但是UDP模式也是最好理解的模式。 UDP mode UDP模式的核心是在每个节点上创建了flannel0网络设备和flanneld守护进程，依靠封包和解包这种隧道模式在节点之间建立起overlay网络。 当本机的pod想发送请求到其他节点上的pod时，由于目的地址不在docker0网段，所以会被docker0网桥发送至宿主机的docker0网卡上，此时查找路由表，由于路由表规则已经被flanneld提前写入，所以会转发到一个叫flannel0的网络设备上，该设备是一个TUN设备，这种设备会将存在于内核态中的网络包转发给创建该网络设备的用户态的进程，在此就是flanneld进程，flanneld进程就会发起一个UDP连接将数据包转发至目标主机，这就是flannel UDP模式的大致流程。其中subnet和宿主机的关系是保存在etcd中。 从网络协议栈的横截面来看的话，从docker0由经路由表转发到flannel0是内核态的操作，flannel0转发给flanneld会从内核态到用户态，而flanneld对数据进行封包以后会再次流过协议栈到eth0网卡转发给其他主机，这其中需要经历三次内核态和用户态之间的复制，严重影响性能。 flannel UDP mode\" flannel UDP mode VXLAN mode VXLAN模式和UDP模式十分类似，都是通过封包和解包的隧道模式构建overlay网络打通节点。 VXLAN模式可以利用Linux内核的VXLAN模块在内核中直接封装和解封装。这就免去了像UDP模式下内核态的flannel0流向用户态的flanneld，然后在flanneld中封包，再流向eth0这个过程。这样就提高了效率，所以VXLAN成为了主流的跨主通信方案。其中VTEP（VXLAN tunnel end point，虚拟隧道端点）就类似flannel0的作用。 VXLAN通信的过程相当复杂，具体见《深入剖析Kubernetes》P264-267。 flannel VXLAN mode\" flannel VXLAN mode host-gw mode host-gw模式是一种三层网络方案，host-gw模式的工作原理非常简单，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。当然，Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。 在 Kubernetes v1.7 之后，类似 Flannel、Calico 的 CNI 网络插件都是可以直接连接 Kubernetes 的 APIServer 来访问 Etcd 的，无需额外部署 Etcd 给它们使用。 而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。host-gw 模式能够正常工作的核心，就在于 IP 包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的 MAC 地址。这样，它就会经过二层网络到达目的宿主机。所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。这样也印证了官方对host-gw的\"Requires direct layer2 connectivity between hosts running flannel.“说法。 ","date":"2022-04-01","objectID":"/posts/22-04-01/:2:1","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"Calico Calico是三层转发网络解决方案的典型代表。 实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。也就是说，Calico 也会在每台宿主机上，添加一个去往下一个node的路由规则。 而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的 IP 地址，找到它所对应的、“下一跳”的网关。不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了BGP路由客户端来自动地在整个集群中分发路由信息。BGP的工作原理可以简单理解为：在每个边界网关上都会运行着一个BGP客户端，它们会将各自的路由表信息，通过 TCP 传输给其他的边界网关。而其他边界网关上的BGP客户端，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里。所谓 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。 Calico项目的架构由三个部分组成： Calico 的 CNI 插件。 Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。 BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。 除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。 Calico\" Calico 需要注意的是，Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N²的规模快速增长，从而给集群本身的网络带来巨大的压力。 所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，则需要用到的是一个叫作 Route Reflector 的模式。 在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。这些专门的节点，就是所谓的 Route Reflector 节点，它们实际上扮演了“中间代理”的角色，从而把 BGP 连接的规模控制在 N 的数量级上。 此外，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。在这种情况下就需要为Calico开启IPIP模式。IPIP模式的含义就是IP in IP，就如同flannel的MAC in UDP一样是通过封包和解包的隧道模式进行传输。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，建议将所有宿主机节点放在一个子网里，避免使用 IPIP。 但是如果 Calico 项目能够让宿主机之间的路由设备（也就是网关），也通过 BGP 协议“学习”到 Calico 网络里的路由规则，那么从容器发出的 IP 包，不就可以通过这些设备路由到目的宿主机了么？但是在 Kubernetes 被广泛使用的公有云场景里，却完全不可行。这里的原因在于：公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置。当然，在大多数公有云环境下，宿主机（公有云提供的虚拟机）本身往往就是二层连通的，所以这个需求也不强烈。不过，在私有部署的环境下，宿主机属于不同子网（VLAN）反而是更加常见的部署状态。这时候，想办法将宿主机网关也加入到 BGP Mesh 里从而避免使用 IPIP，就成了一个非常迫切的需求。而在 Calico 项目中，已经提供了两种将宿主机网关设置成 BGP Peer 的解决方案。 第一种方案需要为BGP配置Dynamic Neighbors，这样就可以给路由器配置一个网段，然后路由器就会自动跟该网段里的主机建立起 BGP Peer 关系。 第二种方案是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过 BGP 协议同步给网关。前面提到，在大规模集群中，Calico 本身就推荐使用 Route Reflector 节点的方式进行组网。所以，这里负责跟宿主机网关进行沟通的独立组件，直接由 Route Reflector 兼任即可。更重要的是，这种情况下网关的 BGP Peer 个数是有限并且固定的。所以就可以直接把这些独立组件配置成路由器的 BGP Peer，而无需 Dynamic Neighbors 的支持。 ","date":"2022-04-01","objectID":"/posts/22-04-01/:2:2","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"网络隔离 在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy。 一个完整的示例如下所示： apiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:test-network-policynamespace:defaultspec:podSelector:matchLabels:role:dbpolicyTypes:- Ingress- Egressingress:- from:- ipBlock:cidr:172.17.0.0/16except:- 172.17.1.0/24- namespaceSelector:matchLabels:project:myproject- podSelector:matchLabels:role:frontendports:- protocol:TCPport:6379egress:- to:- ipBlock:cidr:10.0.0.0/24ports:- protocol:TCPport:5978这里需要明确，Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定。而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。例如，在上面这个例子里，在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。然后，在 ingress 字段里，定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector。而在 egress 字段里，则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。 综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示： 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。 Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。 Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。 而这种策略的本质也是通过iptables来控制的。 ","date":"2022-04-01","objectID":"/posts/22-04-01/:3:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"Service和DNS Kubernetes 从 1.11 版本开始，使用 CoreDNS 替代 KubeDNS 成为了内置的 DNS 服务。 我们创建一个busybox Pod，然后执行 kubectl exec busybox cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 169.254.20.10 options ndots:5 可见搜索域是default.svc.cluster.local、svc.cluster.local和cluster.local DNS满足了服务发现的需求。 之前我在Service简记中描述过ClusterIP和NodePort类型Service的网络原理。Service还有headless、LoadBalancer和ExternalName类型。 LoadBalancer Service类型需要Cloud Provider的支持 Headless Service类型返回该Service选择到的Pod的地址 ExternalName Service可以给一个外部服务起一个内部别名 ","date":"2022-04-01","objectID":"/posts/22-04-01/:4:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"Ingress Ingress对象提供代理七层负载均衡，可以通过设置域名和path的规则访问到内部的Service。 我们可以通过提交Ingress对象设置反向代理规则，但是真正实现反向代理功能的是Ingress-Controller。常见的Ingress-Controller有Nginx、HAproxy和traefik。k3s内置了traefik，traefik的优势就在与热加载。 以常用的Nginx实现为例，当我们提交Ingress，其实就是设置了nginx.conf，就实现了反向代理的功能。Ingress还可以方便的配置TLS。 ","date":"2022-04-01","objectID":"/posts/22-04-01/:5:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Kubernetes"],"content":"总结 k8s的网络非常复杂，这里只是在我阅读《深入剖析Kubernetes》第七章之后的笔记和个人思考，仅仅是整个k8s网络模型中的冰山一角。例如：VXLAN技术其实是在数据中心网络中为了使虚拟机东西迁移的，CNI的调用流程，和最近流行的以eBPF技术为底座，性能十分之高的Cilium插件等等等等都没有涉及。如此复杂的网络产生的原因就是为了屏蔽底层基础设施，让node和node之间没有隔离，Pod可以自由的通过IP地址进行访问，就像在一个单一的巨大的资源大海里一样。 理解k8s网络模型是深入了解k8s的必由之路，只有深刻理解了k8s中的网络才能更好的理解k8s的设计。 lol\" lol ","date":"2022-04-01","objectID":"/posts/22-04-01/:6:0","tags":["Network"],"title":"Kubernetes网络模型","uri":"/posts/22-04-01/"},{"categories":["Golang"],"content":"泛型来了！ ","date":"2022-03-16","objectID":"/posts/22-03-16/:0:0","tags":["Golang"],"title":"Today is the big day!","uri":"/posts/22-03-16/"},{"categories":["Istio"],"content":"注入容器！接管流量！ ","date":"2022-02-28","objectID":"/posts/22-02-28/:0:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio是什么? 《云原生服务网格Istio：原理、实践、架构与源码解析》 概述 云原生容器技术和微服务应用的出现，推动了人们对服务网格的需求。那么，什么是服务网格？简而言之，服务网格是服务（包括微服务）之间通信的控制器。随着越来越多的容器应用的开发和部署，一个企业可能会有成百上千或数以万计的容器在运行，怎样管理这些容器或服务之间的通信，包括服务间的负载均衡、流量管理、路由、运行状况监视、安全策略及服务间身份验证，就成为云原生技术的巨大挑战。以Istio为代表的服务网格应运而生。在架构上，Istio属于云原生技术的基础设施层，通过在容器旁提供一系列网络代理，来实现服务间的通信控制。其中的每个网络代理就是一个网关，管理容器或容器集群中每个服务间的请求或交互。每个网络代理还拦截服务请求分发到服务网格上，因此，众多服务构成的无数连接“编织“成网，也就有了“网格“这个概念。服务网格的中央控制器，在Kubernetes容器平台的帮助下，通过服务策略来控制和调整网络代理的功能，包括收集服务的性能指标。 一句话总结：Istio是一个与Kubernetes紧密结合的适用于云原生场景的Service Mesh形态的用于服务治理的开放平台。 Istio Architecture\" Istio Architecture ","date":"2022-02-28","objectID":"/posts/22-02-28/:1:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio能做什么? Connect Secure Control Observe ","date":"2022-02-28","objectID":"/posts/22-02-28/:2:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"为什么需要Istio? 我们需要解决服务治理问题。 服务治理的演化过程经历了三个阶段： 在微服务程序中直接包含治理逻辑。这就会出现代码重复，维护困难，服务治理逻辑和业务逻辑紧耦合等等问题。 独立治理逻辑，SDK模式。将治理逻辑独立出来，形成类库，例如Spring Cloud Netflix Hystrix和Spring Cloud OpenFeign，我曾经使用过OpenFeign，使用起来也算开箱即用，只需在SpringBoot代码中的调用逻辑处添加@FeignClient和@EnableFeignClients注解即可，这种SDK模式的治理工具集在过去一段时间里得到了非常广泛的应用。虽然SDK模式在代码上解耦了业务和治理逻辑，但业务代码和SDK还是要一起编译的，业务代码和治理逻辑还在一个进程内。这就导致了几个问题：业务代码必须和SDK基于同一种语言，即语言绑定。例如，Spring Cloud等大部分治理框架都基于Java，因此只适用于Java语言开发的服务。并且在治理逻辑升级时，还需要用户的整个服务升级，即使业务逻辑没有改变。 治理逻辑独立的进程。SDK模式仍旧侵入了用户的代码，那就再解耦一层，把治理逻辑彻底从用户的业务代码中剥离出来，这就形成了Sidecar模式。 Sidecar\" Sidecar 《云原生服务网格Istio：原理、实践、架构与源码解析》P12 在云原生时代，随着采用各种语言开发的服务剧增，应用间的访问拓扑更加复杂，治理需求也越来越多。原来的那种嵌入在应用中的治理功能无论是从形态、动态性还是可扩展性来说都不能满足需求，迫切需要一种具备云原生动态、弹性特点的应用治理基础设施。 首先，从单个应用来看，Sidecar与应用进程的解耦带来的应用完全无侵入、开发语言无关等特点解除了开发语言的约束，从而极大降低了应用开发者的开发成本。这种方式经常被称为一种应用的基础设施层，类比TCP/IP网络协议栈，应用程序像使用TCP/IP一样使用这个通用代理：TCP/IP负责将字节码可靠地在网络节点间传递，Sidecar则负责将请求可靠地在服务间进行传递。TCP/IP面向的是底层的数据流，Sidecar则可以支持多种高级协议（HTTP、gRPC、HTTPS等），以及对服务运行时进行高级控制，使服务变得可监控、可管理。 然后，从全局来看，在多个服务间有复杂的互相访问时才有服务治理的需求。即我们关注的是这些Sidecar组成的网格，对网格内的服务间访问进行管理，应用还是按照本来的方式互相访问，每个程序的Inbound流量和Outbound流量都要经过Sidecar代理，并在Sidecar上执行治理动作。 最后，Sidecar是网格动作的执行体，全局的管理规则和网格内的元数据维护通过一个统一的控制面实现，只有数局面的Sidecar和控制面有联系，应用感知不到Sidecar更不会和控制面有任何联系，用户的业务和控制面彻底解耦。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:3:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio的缺点 Istio的Sidecar架构引入了两个问题： 增加了两处延迟和可能的故障点 多出来的这两跳对与访问性能、整体可靠性及整个系统的复杂度都带来了新的挑战。 在云原生环境下需要考虑这点资源损耗吗🤪，一般是通过保证转发代理的轻量和高性能降低时延影响（Istio选择了轻量级的envoy而不是nginx就是处于这个考虑），尤其是考虑到后端实际使用的应用程序一般比代理更重，叠加代理并不会明显影响应用的访问性能；另外，在云原生场景下，多扩展几个replica就行了。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:4:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio和Kubernetes 之前在Service简记这篇文章中，我分析了Service实现的网络原理，可以明确的是k8s的Service已经可以提供服务注册、服务发现和负载均衡功能，并且可以通过服务名直接访问到服务实例（DNS）。为什么还是需要Istio呢？本质上来看，是因为k8s的Service只提供了OSI的4层负载均衡能力，无法基于应用层的信息进行负载均衡，不会提供应用层的流量管理，更不会提供服务访问指标和调用链追踪这种应用的服务运行诊断能力。但是要问到为什么不使用k8s的Ingress资源呢？Ingress不是也是可以提供7层的路由和负载均衡吗？个人认为Ingress主要提供了k8s集群南北流量的管理（Ingress Controller的实现主要采用Nginx），而Istio对容器注入的proxy更加细粒度的控制了k8s集群内的东西流量（proxy的实现是envoy)。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:5:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio小实践 ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"部署 以官方的Bookinfo为示例，略过安装步骤。 Bookinfo\" Bookinfo 首先为想要实现Istio边车自动注入功能的命名空间打上标签kubectl label namespace default istio-injection=enabled，这样在该名称空间内的pod会被自动注入proxy。 然后部署Bookinfo应用kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml。 之后kubectl get pods -n istio-system可以看到 ingress、egress和istiod这三个pod，这就是Istio的控制平面，而且是以pod的形式存在的，可见Istio和k8s的结合十分紧密，Istio就是附加在k8s上的。 其他四个pod是之后部署的，都是用来Observe，从创建时间可以看出。 istio-system namespace\" istio-system namespace 再看默认的名称空间，有六个pod，且每个pod里有两个容器。 default namespace\" default namespace kubectl describe pod productpage-v1-65b75f6885-qgnzd可以看到 这个productpage-v1-65b75f6885-qgnzdpod中有本身的productpage容器还有istio/proxyv2，这个就是被自动注入的proxy，就是流量控制的实际执行者。 istio/proxyv2\" istio/proxyv2 当我们向kube-apiserver提交一个网络策略时，时刻watch着api-server动向的Istio控制平面的istiod得知了这个消息，然后分发到各个被注入的pod的proxy容器（envoy）中，然后envoy加载网络策略并执行，这就是istio的实现机制。 进行完以上步骤之后再对外开放服务，添加一系列Observe组件等等一系列过程不再赘述。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:1","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"测试 在本地机器上执行脚本模拟请求 while true do curl http://$INGRESS_HOST:$INGRESS_PORT/productpage done 之后就可以在kiali这个观测平台上查看流量 kiali\" kiali ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:2","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"总结 Istio与Kubernetes的天然融合且基于Kubernetes构建，补齐了Kubernetes的治理能力，提供了端到端的服务运行治理平台。这都使得Istio、微服务、容器及Kubernetes形成一个完美的闭环。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:7:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Kubernetes"],"content":"Service是虚拟的！ ","date":"2022-02-10","objectID":"/posts/22-02-10/:0:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service是什么 Pod中的容器可以通过pause容器共享网络命名空间实现互访 同一个Node内的不同Pod可以通过Docker0网桥实现互访 不同Node上的Pod可以通过CNI插件，例如Flannel实现互访 通过以上三点，一个覆盖了k8s集群的扁平化Pod互访网络就建立起来了，这个Overlay Network中Pod的IP地址由Flannel进行划分。 但是Pod的生命周期是短暂的，Pod的死亡、Pod重新被拉起、Pod的扩容都会导致IP地址的变化。所以不能在Pod内直接使用IP地址来进行Pod间的访问。我们需要一个持久的静态IP来实现Pod间的访问，同时实现负载均衡和服务发现。Service是一个抽象出来的k8s资源，是由每个worker node上的kube-proxy组件通过修改iptables（iptables mode）来实现的。 可以将Service理解为一个虚拟IP。Service的IP空间又是独立于Pod的IP和Node的IP，可以被人为指定，所以一般来讲k8s的网络空间被分为三层。Pod的IP是由Flannel划分的，一个Node上一个子空间；Node的IP就是真实物理网卡IP；再加上Service的IP，组成三层网络模型。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:1:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service如何实现 Service是由kube-proxy组件通过修改iptables实现的。 wikipedia iptables是运行在用户空间的应用软件，通过控制Linux内核netfilter模块，来管理网络数据包的处理和转发。 iptables Process Flow\" iptables Process Flow kube-proxy主要设置了iptables中的nat表，通过在nat表中设置PREROUTING和OUTPUT链来劫持网络数据，负载均衡地导向至Service选择到的Pod。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:2:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"一个例子 具体的yaml就不展示了，清单里有一个Service，Service选择一个Pod，Pod有8080、1985、1935三个端口。 此时使用iptables -t nat -nvL查看nat表中的所有链，发现PREROUTING和OUTPUT链均在头部插入了一个KUBE-SERVICES。这个KUBE-SERVICES链就是k8s的Service的门户，Service就是通过这个链劫持流量，进行DNAT和SNAT完成Pod间的互访。 通过kubectl get svc可以查看到Service的Cluster IP是10.43.69.202。接着可以跟踪KUBE-SERVICES。如下 KUBE-SERVICES链\" KUBE-SERVICES链 可以看出所有目的地为10.43.69.202（也就是Service的Cluster IP）且目的端口为1935的数据包都被导向至KUBE-SVC-FQTBCD7TKAEMWX5M链，可以理解为就是Service。接着跟踪KUBE-SVC-FQTBCD7TKAEMWX5M链。如下 KUBE-SVC-FQTBCD7TKAEMWX5M链\" KUBE-SVC-FQTBCD7TKAEMWX5M链 发现只有一个KUBE-SEP-CQBFNZNKSCBZ27C2链，SEP意思就是Service End Point。因为我的Service下只有一个Pod所以这个地方只有一个SEP链，如果Service选择了多个Pod的话，这里应该有对应的endPoint数量的SEP链数，并且使用iptables statistic 模块做round-robin的负载均衡。其实看到这里就可以发现使用iptables模式实现Service的一些不足，例如：如果创建了许多的Service，那么iptables的链数就会变得肿胀，性能就会下降。而且iptables只有RR这一种负载均衡策略，但现实是我们需要更多的负载均衡策略，于是在Kubernetes1.14版本以后，就默认使用ipvs模式来替代iptables模式。 继续跟进KUBE-SEP-CQBFNZNKSCBZ27C2链可以发现 KUBE-SEP-CQBFNZNKSCBZ27C2链\" KUBE-SEP-CQBFNZNKSCBZ27C2链 最后的DNAT，也就是目的地址转换，将目的地址从最初的虚拟的Service Cluster IP转换至Pod的IP和目的端口1935。 NodePort类型 Service常见的有类型ClusterIP（默认）、NodePort和Load Balancer。 在常见的情况下，集群内Pod间的互访是通过设置Service Cluster IP命中OUTPUT链最终被导向Pod。 而集群外的请求进入集群内则需要将Service设置为NodePort（生产环境中不这么做），kube-proxy会在每个worker node上开启端口，外部请求会命中PREROUTING链最后的KUBE-NODEPORTS链，匹配规则是\"ADDRTYPE match dst-type LOCAL\"。之后会继续根据端口导向至对应的Service链。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:3:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"总结 k8s中的Service是一个抽象出来的功能概念。 我们可以将Service想象成为一个接口，它定义了几个功能： 有贯穿Service生命周期的固定的IP和端口 服务发现 负载均衡 而iptables模式是对Service的一个具体实现： kube-proxy通过设置iptables对流量进行转发 kube-proxy通过watch api-server（是否有Pod死亡？重新拉起？扩容？）来改写iptables规则 在iptables的SVC链中对SEP链进行RR的负载均衡 iptables实现Service可以用一张图总结: 通过iptables实现Service\" 通过iptables实现Service 所以我们可以说Service是通过kube-proxy和iptables联合实现的，是为Pod创造出来的假象。 参考： https://www.jianshu.com/p/67744d680286 https://www.cnblogs.com/charlieroro/p/9588019.html ","date":"2022-02-10","objectID":"/posts/22-02-10/:4:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"听说kube-prometheus开箱即用？ ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:0:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"使用kind创建集群 安装好kind后直接kind create cluster --config config.yaml --name cluster启动集群，config.yaml如下，是一个有一个master两个node的测试集群。测试完成后kind delete cluster --name cluster即可删除集群。 # three node (two workers) cluster configkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4nodes:- role:control-plane- role:worker- role:worker在尝试过minikube和kind后还是选择了kind。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:1:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"部署kube-prometheus 一般在集群中部署Prometheus有三种方法：Prometheus Operator、kube-prometheus 和 community helm chart。 部署方式比较 Prometheus Operator The Prometheus Operator uses Kubernetes custom resources to simplify the deployment and configuration of Prometheus, Alertmanager, and related monitoring components. kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. This chart is maintained by the Prometheus community. For more information, please see the chart’s readme kube-prometheus提供了许多开箱即用的配置，例如Alertmanager、node-exporter以及Grafana dashboards。 将kube-prometheus项目clone至本地，然后执行: # Create the namespace and CRDs, and then wait for them to be available before creating the remaining resources kubectl create -f manifests/setup until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done kubectl create -f manifests/ 由于创建镜像需要拉取的镜像托管在quay.io，会出现ImagePullBackOff错误： ImagePullBackOff\" ImagePullBackOff k describe pods -n monitoring prometheus-operator-75d9b475d9-f29g8检查一下，是 quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 两个镜像拉取失败，这时借助阿里云容器镜像服务进行拉取。 阿里云容器镜像服务构建镜像步骤 创建GitHub repo，路径格式为：image-name/tag/Dockerfile 在阿里云容器镜像服务中创建仓库image-name 选择代码源为GitHub repo，选择海外构建 进入仓库，添加构建规则，立即构建 构建完成后登录到阿里云容器镜像服务： docker login --username=xxx registry.cn-hangzhou.aliyuncs.com 拉取构建好的镜像：docker pull registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 进行retag：docker tag registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 删除之前的镜像：docker rmi registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 brancz/kube-rbac-proxy:v0.11.0也重复上述步骤，此时就有了kube-operator所需的镜像。 使用kind load docker-image quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 --name cluster将镜像加载进kind集群，然后prometheus-operator-75d9b475d9-f29g8这个pod就正常运行了。 kube-operator-pod处于Running状态\" kube-operator-pod处于Running状态 若还有镜像报错，重复上述步骤，直到所有pod都处于Running状态，到此kube-prometheus部署成功。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:2:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"测试kube-prometheus ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"Prometheus 这里设置service的端口转发，接着就能进入Prometheus的UI尝试PromQL了。 kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 info 一本Prometheus的书 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:1","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"grafana 同样设置端口转发 kubectl --namespace monitoring port-forward svc/grafana 3000 tip 用户名和密码都是admin 之后就可以尝试自带的面板了(例如node-exporter)： grafana node-exporter\" grafana node-exporter Grafana不仅可以对接Prometheus，还可以对接zabbix等很多数据源，是一个非常漂亮和好用的可视化工具。 info 还有一个自称Kubernetes IDE的工具Lens，我尝试了一下，也很好用。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:2","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":null,"content":"About Me? ⚽ 希望朗尼克下课的曼联球迷 || FIFA边路传中选手 || 防守反击战术拥趸 || 小区级后腰 🏓 初中退役选手 🎵 王若琳 || Nujabes || John Coltrane || Slipknot || Bob Marley || Blur || 一些南美民歌 🎬 gakki || 広末 涼子 || 満島ひかり 🐾 PGP指纹 : 07B8 C554 5047 05F5 53A8 0A9D 51B3 76E1 190C B0D7 🍺☕ and... FUCK GFW ","date":"2021-08-03","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Website Timeline NAME READY STATUS RESTARTS ROLES this is my etcd. 1/1 Running 0 control-panel ","date":"2021-08-03","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Some Configurations init.vim \" ##########基本配置########## set nu set clipboard=unnamedplus set termguicolors syntax enable set tabstop=4 set softtabstop=4 set shiftwidth=4 set noexpandtab set autoindent set cursorline set relativenumber \" ##########按键映射########## :let mapleader=\",\" \" ##########插件配置########## call plug#begin() Plug 'arcticicestudio/nord-vim' Plug 'mhinz/vim-startify' Plug 'scrooloose/nerdtree' Plug 'junegunn/fzf', { 'do': { -\u003e fzf#install() } } Plug 'fatih/vim-go', { 'do': ':GoUpdateBinaries' } call plug#end() \" NERDTree nnoremap \u003cleader\u003en :NERDTree\u003cCR\u003e nnoremap \u003cleader\u003et :NERDTreeToggle\u003cCR\u003e nnoremap \u003cleader\u003ef :NERDTreeFind\u003cCR\u003e \" vim-go \" https://github.com/arcticicestudio/nord-vim/issues/203#issuecomment-620800009 let g:go_highlight_structs = 1 let g:go_highlight_methods = 1 let g:go_highlight_functions = 1 \" diff from the issue is ADD THIS LINE let g:go_highlight_function_calls = 1 let g:go_highlight_function_parameters = 1 let g:go_highlight_operators = 1 let g:go_highlight_types = 1 let g:go_highlight_fields = 1 let g:go_highlight_build_constraints = 1 let g:go_highlight_generate_tags = 1 let g:go_highlight_format_strings = 1 let g:go_highlight_variable_declarations = 1 let g:go_highlight_variable_assignments = 1 let g:go_auto_type_info =1 let g:go_fmt_autosave = 1 let g:go_mod_fmt_autosave = 1 let g:go_gopls_enabled = 1 \" ##########主题配置########## let g:nord_uniform_status_lines = 1 colorscheme nord ","date":"2021-08-03","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Hugo"],"content":"博客还是简洁一点比较好？ ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:0:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"效果 git commit + push即可。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:1:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"workflow 如下图: workflow(点击图片跳转至Live Editor) 上图是用mermaid画的，flowchart是mermaid高版本的feature，在我使用Feelit主题shortcode的时候还没有得到支持，提了个issue，在这次更新中得到了支持。 mermaid shortcode在手机浏览器中没有得到很好的支持，换回figure shortcode。(2021年 08月 04日 星期三 15:30:15 CST) ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用Docker进行本地预览 参考了jojomi/docker-hugo重写了Dockerfile和构建镜像并且启动容器的脚本chengleqi/docker-hugo。 镜像的README 在浏览器中下载最新版hugo binary(extend版本) 将下载好的hugo binary放在项目根目录 执行./build_and_start_hugo.sh 构建镜像并运行容器 打开浏览器 localhost:1313 注意 在docker run的时候挂载了blog目录用于hugo server，并添加了--rm参数，退出容器后直接删除容器。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:1","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用GitHub Actions自动构建并部署 此处参考了Arnab Kumar Shil的文章。配置了GitHub Actions的流水线。脚本如下: name:github pageson:push:branches:- main # Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-20.04steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'latest'extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.ref == 'refs/heads/main'with:personal_token:${{ secrets.ACTION_ACCESS_TOKEN }}external_repository:chengleqi/chengleqi.github.iopublish_branch:mainpublish_dir:./publiccname:chengleqi.com这个脚本使用了peaceiris/actions-hugo@v2和peaceiris/actions-gh-pages@v3两个action分别用于设置hugo环境以及推送至gh-pages。 技巧 存储blog源文件的repo访问github.io repo需要设置personal_token: ${{ secrets.ACTION_ACCESS_TOKEN }} 在Deploy job中需要指定CNAME，会自动生成CNAME文件，当然也可以手动添加，参考Hugo官网。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:2","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"启用algolia搜索 因为algolia需要上传索引文件，主题的作者推荐了一款Algolia Atomic插件用来完成自动化上传。 这款插件需要node和npm，我参考了这篇文章，直接在GitHub Actions分配的VPS中配置不成功。 于是找到了这个方案，作者将atomic-algolia封装进docker容器中，启动立即执行atomic-aloglia。将docker启动命令附加到pipeline脚本中，再次推送，一次成功，只是多出了拉取镜像的时间。我已将镜像同步至Docker Hub进行解耦。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:3","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"}]