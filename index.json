[{"categories":["Golang"],"content":"泛型来了！ ","date":"2022-03-16","objectID":"/posts/22-03-16/:0:0","tags":["Golang"],"title":"Today is the big day!","uri":"/posts/22-03-16/"},{"categories":["Istio"],"content":"注入容器！接管流量！ ","date":"2022-02-28","objectID":"/posts/22-02-28/:0:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio是什么? 《云原生服务网格Istio：原理、实践、架构与源码解析》 概述 云原生容器技术和微服务应用的出现，推动了人们对服务网格的需求。那么，什么是服务网格？简而言之，服务网格是服务（包括微服务）之间通信的控制器。随着越来越多的容器应用的开发和部署，一个企业可能会有成百上千或数以万计的容器在运行，怎样管理这些容器或服务之间的通信，包括服务间的负载均衡、流量管理、路由、运行状况监视、安全策略及服务间身份验证，就成为云原生技术的巨大挑战。以Istio为代表的服务网格应运而生。在架构上，Istio属于云原生技术的基础设施层，通过在容器旁提供一系列网络代理，来实现服务间的通信控制。其中的每个网络代理就是一个网关，管理容器或容器集群中每个服务间的请求或交互。每个网络代理还拦截服务请求分发到服务网格上，因此，众多服务构成的无数连接“编织“成网，也就有了”网格“这个概念。服务网格的中央控制器，在Kubernetes容器平台的帮助下，通过服务策略来控制和调整网络代理的功能，包括收集服务的性能指标。 一句话总结：Istio是一个与Kubernetes紧密结合的适用于云原生场景的Service Mesh形态的用于服务治理的开放平台。 Istio Architecture\" Istio Architecture ","date":"2022-02-28","objectID":"/posts/22-02-28/:1:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio能做什么? Connect Secure Control Observe ","date":"2022-02-28","objectID":"/posts/22-02-28/:2:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"为什么需要Istio? 我们需要解决服务治理问题。 服务治理的演化过程经历了三个阶段： 在微服务程序中直接包含治理逻辑。这就会出现代码重复，维护困难，服务治理逻辑和业务逻辑紧耦合等等问题。 独立治理逻辑，SDK模式。将治理逻辑独立出来，形成类库，例如Spring Cloud Netflix Hystrix和Spring Cloud OpenFeign，我曾经使用过OpenFeign，使用起来也算开箱即用，只需在SpringBoot代码中的调用逻辑处添加@FeignClient和@EnableFeignClients注解即可，这种SDK模式的治理工具集在过去一段时间里得到了非常广泛的应用。虽然SDK模式在代码上解耦了业务和治理逻辑，但业务代码和SDK还是要一起编译的，业务代码和治理逻辑还在一个进程内。这就导致了几个问题：业务代码必须和SDK基于同一种语言，即语言绑定。例如，Spring Cloud等大部分治理框架都基于Java，因此只适用于Java语言开发的服务。并且在治理逻辑升级时，还需要用户的整个服务升级，即使业务逻辑没有改变。 治理逻辑独立的进程。SDK模式仍旧侵入了用户的代码，那就再解耦一层，把治理逻辑彻底从用户的业务代码中剥离出来，这就形成了Sidecar模式。 Sidecar\" Sidecar 《云原生服务网格Istio：原理、实践、架构与源码解析》P12 在云原生时代，随着采用各种语言开发的服务剧增，应用间的访问拓扑更加复杂，治理需求也越来越多。原来的那种嵌入在应用中的治理功能无论是从形态、动态性还是可扩展性来说都不能满足需求，迫切需要一种具备云原生动态、弹性特点的应用治理基础设施。 首先，从单个应用来看，Sidecar与应用进程的解耦带来的应用完全无侵入、开发语言无关等特点解除了开发语言的约束，从而极大降低了应用开发者的开发成本。这种方式经常被称为一种应用的基础设施层，类比TCP/IP网络协议栈，应用程序像使用TCP/IP一样使用这个通用代理：TCP/IP负责将字节码可靠地在网络节点间传递，Sidecar则负责将请求可靠地在服务间进行传递。TCP/IP面向的是底层的数据流，Sidecar则可以支持多种高级协议（HTTP、gRPC、HTTPS等），以及对服务运行时进行高级控制，使服务变得可监控、可管理。 然后，从全局来看，在多个服务间有复杂的互相访问时才有服务治理的需求。即我们关注的是这些Sidecar组成的网格，对网格内的服务间访问进行管理，应用还是按照本来的方式互相访问，每个程序的Inbound流量和Outbound流量都要经过Sidecar代理，并在Sidecar上执行治理动作。 最后，Sidecar是网格动作的执行体，全局的管理规则和网格内的元数据维护通过一个统一的控制面实现，只有数局面的Sidecar和控制面有联系，应用感知不到Sidecar更不会和控制面有任何联系，用户的业务和控制面彻底解耦。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:3:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio的缺点 Istio的Sidecar架构引入了两个问题： 增加了两处延迟和可能的故障点 多出来的这两跳对与访问性能、整体可靠性及整个系统的复杂度都带来了新的挑战。 在云原生环境下需要考虑这点资源损耗吗🤪，一般是通过保证转发代理的轻量和高性能降低时延影响（Istio选择了轻量级的envoy而不是nginx就是处于这个考虑），尤其是考虑到后端实际使用的应用程序一般比代理更重，叠加代理并不会明显影响应用的访问性能；另外，在云原生场景下，多扩展几个replica就行了。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:4:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio和Kubernetes 之前在Service简记这篇文章中，我分析了Service实现的网络原理，可以明确的是k8s的Service已经可以提供服务注册、服务发现和负载均衡功能，并且可以通过服务名直接访问到服务实例（DNS）。为什么还是需要Istio呢？本质上来看，是因为k8s的Service只提供了OSI的4层负载均衡能力，无法基于应用层的信息进行负载均衡，不会提供应用层的流量管理，更不会提供服务访问指标和调用链追踪这种应用的服务运行诊断能力。但是要问到为什么不使用k8s的Ingress资源呢？Ingress不是也是可以提供7层的路由和负载均衡吗？个人认为Ingress主要提供了k8s集群南北流量的管理（Ingress Controller的实现主要采用Nginx），而Istio对容器注入的proxy更加细粒度的控制了k8s集群内的东西流量（proxy的实现是envoy)。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:5:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"Istio小实践 ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"部署 以官方的Bookinfo为示例，略过安装步骤。 Bookinfo\" Bookinfo 首先为想要实现Istio边车自动注入功能的命名空间打上标签kubectl label namespace default istio-injection=enabled，这样在该名称空间内的pod会被自动注入proxy。 然后部署Bookinfo应用kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml。 之后kubectl get pods -n istio-system可以看到 ingress、egress和istiod这三个pod，这就是Istio的控制平面，而且是以pod的形式存在的，可见Istio和k8s的结合十分紧密，Istio就是附加在k8s上的。 其他四个pod是之后部署的，都是用来Observe，从创建时间可以看出。 istio-system namespace\" istio-system namespace 再看默认的名称空间，有六个pod，且每个pod里有两个容器。 default namespace\" default namespace kubectl describe pod productpage-v1-65b75f6885-qgnzd可以看到 这个productpage-v1-65b75f6885-qgnzdpod中有本身的productpage容器还有istio/proxyv2，这个就是被自动注入的proxy，就是流量控制的实际执行者。 istio/proxyv2\" istio/proxyv2 当我们向kube-apiserver提交一个网络策略时，时刻watch着api-server动向的Istio控制平面的istiod得知了这个消息，然后分发到各个被注入的pod的proxy容器（envoy）中，然后envoy加载网络策略并执行，这就是istio的实现机制。 进行完以上步骤之后再对外开放服务，添加一系列Observe组件等等一系列过程不再赘述。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:1","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"测试 在本地机器上执行脚本模拟请求 while true do curl http://$INGRESS_HOST:$INGRESS_PORT/productpage done 之后就可以在kiali这个观测平台上查看流量 kiali\" kiali ","date":"2022-02-28","objectID":"/posts/22-02-28/:6:2","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Istio"],"content":"总结 Istio与Kubernetes的天然融合且基于Kubernetes构建，补齐了Kubernetes的治理能力，提供了端到端的服务运行治理平台。这都使得Istio、微服务、容器及Kubernetes形成一个完美的闭环。 ","date":"2022-02-28","objectID":"/posts/22-02-28/:7:0","tags":["Network"],"title":"服务网格简记","uri":"/posts/22-02-28/"},{"categories":["Kubernetes"],"content":"Service是虚拟的！ ","date":"2022-02-10","objectID":"/posts/22-02-10/:0:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service是什么 Pod中的容器可以通过pause容器共享网络命名空间实现互访 同一个Node内的不同Pod可以通过Docker0网桥实现互访 不同Node上的Pod可以通过CNI插件，例如Flannel实现互访 通过以上三点，一个覆盖了k8s集群的扁平化Pod互访网络就建立起来了，这个Overlay Network中Pod的IP地址由Flannel进行划分。 但是Pod的生命周期是短暂的，Pod的死亡、Pod重新被拉起、Pod的扩容都会导致IP地址的变化。所以不能在Pod内直接使用IP地址来进行Pod间的访问。我们需要一个持久的静态IP来实现Pod间的访问，同时实现负载均衡和服务发现。Service是一个抽象出来的k8s资源，是由每个worker node上的kube-proxy组件通过修改iptables（iptables mode）来实现的。 可以将Service理解为一个虚拟IP。Service的IP空间又是独立于Pod的IP和Node的IP，可以被人为指定，所以一般来讲k8s的网络空间被分为三层。Pod的IP是由Flannel划分的，一个Node上一个子空间；Node的IP就是真实物理网卡IP；再加上Service的IP，组成三层网络模型。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:1:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service如何实现 Service是由kube-proxy组件通过修改iptables实现的。 wikipedia iptables是运行在用户空间的应用软件，通过控制Linux内核netfilter模块，来管理网络数据包的处理和转发。 iptables Process Flow\" iptables Process Flow kube-proxy主要设置了iptables中的nat表，通过在nat表中设置PREROUTING和OUTPUT链来劫持网络数据，负载均衡地导向至Service选择到的Pod。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:2:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"一个例子 具体的yaml就不展示了，清单里有一个Service，Service选择一个Pod，Pod有8080、1985、1935三个端口。 此时使用iptables -t nat -nvL查看nat表中的所有链，发现PREROUTING和OUTPUT链均在头部插入了一个KUBE-SERVICES。这个KUBE-SERVICES链就是k8s的Service的门户，Service就是通过这个链劫持流量，进行DNAT和SNAT完成Pod间的互访。 通过kubectl get svc可以查看到Service的Cluster IP是10.43.69.202。接着可以跟踪KUBE-SERVICES。如下 KUBE-SERVICES链\" KUBE-SERVICES链 可以看出所有目的地为10.43.69.202（也就是Service的Cluster IP）且目的端口为1935的数据包都被导向至KUBE-SVC-FQTBCD7TKAEMWX5M链，可以理解为就是Service。接着跟踪KUBE-SVC-FQTBCD7TKAEMWX5M链。如下 KUBE-SVC-FQTBCD7TKAEMWX5M链\" KUBE-SVC-FQTBCD7TKAEMWX5M链 发现只有一个KUBE-SEP-CQBFNZNKSCBZ27C2链，SEP意思就是Service End Point。因为我的Service下只有一个Pod所以这个地方只有一个SEP链，如果Service选择了多个Pod的话，这里应该有对应的endPoint数量的SEP链数，并且使用iptables statistic 模块做round-robin的负载均衡。其实看到这里就可以发现使用iptables模式实现Service的一些不足，例如：如果创建了许多的Service，那么iptables的链数就会变得肿胀，性能就会下降。而且iptables只有RR这一种负载均衡策略，但现实是我们需要更多的负载均衡策略，于是在Kubernetes1.14版本以后，就默认使用ipvs模式来替代iptables模式。 继续跟进KUBE-SEP-CQBFNZNKSCBZ27C2链可以发现 KUBE-SEP-CQBFNZNKSCBZ27C2链\" KUBE-SEP-CQBFNZNKSCBZ27C2链 最后的DNAT，也就是目的地址转换，将目的地址从最初的虚拟的Service Cluster IP转换至Pod的IP和目的端口1935。 NodePort类型 Service常见的有类型ClusterIP（默认）、NodePort和Load Balancer。 在常见的情况下，集群内Pod间的互访是通过设置Service Cluster IP命中OUTPUT链最终被导向Pod。 而集群外的请求进入集群内则需要将Service设置为NodePort（生产环境中不这么做），kube-proxy会在每个worker node上开启端口，外部请求会命中PREROUTING链最后的KUBE-NODEPORTS链，匹配规则是\"ADDRTYPE match dst-type LOCAL\"。之后会继续根据端口导向至对应的Service链。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:3:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"总结 k8s中的Service是一个抽象出来的功能概念。 我们可以将Service想象成为一个接口，它定义了几个功能： 有贯穿Service生命周期的固定的IP和端口 服务发现 负载均衡 而iptables模式是对Service的一个具体实现： kube-proxy通过设置iptables对流量进行转发 kube-proxy通过watch api-server（是否有Pod死亡？重新拉起？扩容？）来改写iptables规则 在iptables的SVC链中对SEP链进行RR的负载均衡 iptables实现Service可以用一张图总结: 通过iptables实现Service\" 通过iptables实现Service 所以我们可以说Service是通过kube-proxy和iptables联合实现的，是为Pod创造出来的假象。 参考： https://www.jianshu.com/p/67744d680286 https://www.cnblogs.com/charlieroro/p/9588019.html ","date":"2022-02-10","objectID":"/posts/22-02-10/:4:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["IaC"],"content":"可能是要鸽了？ 《What is Infrastructure as Code (IaC)?》by RedHat Infrastructure as Code (IaC) is the managing and provisioning of infrastructure through code instead of through manual processes. With IaC, configuration files are created that contain your infrastructure specifications, which makes it easier to edit and distribute configurations. It also ensures that you provision the same environment every time. By codifying and documenting your configuration specifications, IaC aids configuration management and helps you to avoid undocumented, ad-hoc configuration changes. Version control is an important part of IaC, and your configuration files should be under source control just like any other software source code file. Deploying your infrastructure as code also means that you can divide your infrastructure into modular components that can then be combined in different ways through automation. Automating infrastructure provisioning with IaC means that developers don’t need to manually provision and manage servers, operating systems, storage, and other infrastructure components each time they develop or deploy an application. Codifying your infrastructure gives you a template to follow for provisioning, and although this can still be accomplished manually, an automation tool, such as Red Hat® Ansible Automation® Platform, can do it for you. ","date":"2021-10-05","objectID":"/posts/21-10-05/:0:0","tags":["Terraform","Pulumi"],"title":"IaC: 从Terraform到Pulumi","uri":"/posts/21-10-05/"},{"categories":["Kubernetes"],"content":"听说kube-prometheus开箱即用？ ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:0:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"使用kind创建集群 安装好kind后直接kind create cluster --config config.yaml --name cluster启动集群，config.yaml如下，是一个有一个master两个node的测试集群。测试完成后kind delete cluster --name cluster即可删除集群。 # three node (two workers) cluster configkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4nodes:- role:control-plane- role:worker- role:worker在尝试过minikube和kind后还是选择了kind。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:1:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"部署kube-prometheus 一般在集群中部署Prometheus有三种方法：Prometheus Operator、kube-prometheus 和 community helm chart。 部署方式比较 Prometheus Operator The Prometheus Operator uses Kubernetes custom resources to simplify the deployment and configuration of Prometheus, Alertmanager, and related monitoring components. kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. This chart is maintained by the Prometheus community. For more information, please see the chart’s readme kube-prometheus提供了许多开箱即用的配置，例如Alertmanager、node-exporter以及Grafana dashboards。 将kube-prometheus项目clone至本地，然后执行: # Create the namespace and CRDs, and then wait for them to be available before creating the remaining resources kubectl create -f manifests/setup until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done kubectl create -f manifests/ 由于创建镜像需要拉取的镜像托管在quay.io，会出现ImagePullBackOff错误： ImagePullBackOff\" ImagePullBackOff k describe pods -n monitoring prometheus-operator-75d9b475d9-f29g8检查一下，是 quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 两个镜像拉取失败，这时借助阿里云容器镜像服务进行拉取。 阿里云容器镜像服务构建镜像步骤 创建GitHub repo，路径格式为：image-name/tag/Dockerfile 在阿里云容器镜像服务中创建仓库image-name 选择代码源为GitHub repo，选择海外构建 进入仓库，添加构建规则，立即构建 构建完成后登录到阿里云容器镜像服务： docker login --username=xxx registry.cn-hangzhou.aliyuncs.com 拉取构建好的镜像：docker pull registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 进行retag：docker tag registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 删除之前的镜像：docker rmi registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 brancz/kube-rbac-proxy:v0.11.0也重复上述步骤，此时就有了kube-operator所需的镜像。 使用kind load docker-image quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 --name cluster将镜像加载进kind集群，然后prometheus-operator-75d9b475d9-f29g8这个pod就正常运行了。 kube-operator-pod处于Running状态\" kube-operator-pod处于Running状态 若还有镜像报错，重复上述步骤，直到所有pod都处于Running状态，到此kube-prometheus部署成功。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:2:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"测试kube-prometheus ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"Prometheus 这里设置service的端口转发，接着就能进入Prometheus的UI尝试PromQL了。 kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 info 一本Prometheus的书 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:1","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"grafana 同样设置端口转发 kubectl --namespace monitoring port-forward svc/grafana 3000 tip 用户名和密码都是admin 之后就可以尝试自带的面板了(例如node-exporter)： grafana node-exporter\" grafana node-exporter Grafana不仅可以对接Prometheus，还可以对接zabbix等很多数据源，是一个非常漂亮和好用的可视化工具。 info 还有一个自称Kubernetes IDE的工具Lens，我尝试了一下，也很好用。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:2","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":null,"content":"About Me? ⚽ 希望朗尼克下课的曼联球迷 || FIFA边路传中选手 || 防守反击战术拥趸 || 小区级后腰 🏓 初中退役选手 🎵 王若琳 || Nujabes || John Coltrane || Slipknot || Bob Marley || Blur || 一些南美民歌 🎬 gakki || 広末 涼子 || 満島ひかり 🐾 PGP指纹 : 07B8 C554 5047 05F5 53A8 0A9D 51B3 76E1 190C B0D7 🍺☕ and... FUCK GFW ","date":"2021-08-03","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Website Timeline NAME READY STATUS RESTARTS ROLES this is my etcd. 1/1 Running 0 control-panel ","date":"2021-08-03","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Some Configurations init.vim \" ##########基本配置########## set nu set clipboard=unnamedplus set termguicolors syntax enable set tabstop=4 set softtabstop=4 set shiftwidth=4 set noexpandtab set autoindent set cursorline set relativenumber \" ##########按键映射########## :let mapleader=\",\" \" ##########插件配置########## call plug#begin() Plug 'arcticicestudio/nord-vim' Plug 'mhinz/vim-startify' Plug 'scrooloose/nerdtree' Plug 'junegunn/fzf', { 'do': { -\u003e fzf#install() } } Plug 'fatih/vim-go', { 'do': ':GoUpdateBinaries' } call plug#end() \" NERDTree nnoremap \u003cleader\u003en :NERDTree\u003cCR\u003e nnoremap \u003cleader\u003et :NERDTreeToggle\u003cCR\u003e nnoremap \u003cleader\u003ef :NERDTreeFind\u003cCR\u003e \" vim-go \" https://github.com/arcticicestudio/nord-vim/issues/203#issuecomment-620800009 let g:go_highlight_structs = 1 let g:go_highlight_methods = 1 let g:go_highlight_functions = 1 \" diff from the issue is ADD THIS LINE let g:go_highlight_function_calls = 1 let g:go_highlight_function_parameters = 1 let g:go_highlight_operators = 1 let g:go_highlight_types = 1 let g:go_highlight_fields = 1 let g:go_highlight_build_constraints = 1 let g:go_highlight_generate_tags = 1 let g:go_highlight_format_strings = 1 let g:go_highlight_variable_declarations = 1 let g:go_highlight_variable_assignments = 1 let g:go_auto_type_info =1 let g:go_fmt_autosave = 1 let g:go_mod_fmt_autosave = 1 let g:go_gopls_enabled = 1 \" ##########主题配置########## let g:nord_uniform_status_lines = 1 colorscheme nord ","date":"2021-08-03","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Hugo"],"content":"博客还是简洁一点比较好？ ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:0:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"效果 git commit + push即可。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:1:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"workflow 如下图: workflow(点击图片跳转至Live Editor) 上图是用mermaid画的，flowchart是mermaid高版本的feature，在我使用Feelit主题shortcode的时候还没有得到支持，提了个issue，在这次更新中得到了支持。 mermaid shortcode在手机浏览器中没有得到很好的支持，换回figure shortcode。(2021年 08月 04日 星期三 15:30:15 CST) ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用Docker进行本地预览 参考了jojomi/docker-hugo重写了Dockerfile和构建镜像并且启动容器的脚本chengleqi/docker-hugo。 镜像的README 在浏览器中下载最新版hugo binary(extend版本) 将下载好的hugo binary放在项目根目录 执行./build_and_start_hugo.sh 构建镜像并运行容器 打开浏览器 localhost:1313 注意 在docker run的时候挂载了blog目录用于hugo server，并添加了--rm参数，退出容器后直接删除容器。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:1","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用GitHub Actions自动构建并部署 此处参考了Arnab Kumar Shil的文章。配置了GitHub Actions的流水线。脚本如下: name:github pageson:push:branches:- main # Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-20.04steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'latest'extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.ref == 'refs/heads/main'with:personal_token:${{ secrets.ACTION_ACCESS_TOKEN }}external_repository:chengleqi/chengleqi.github.iopublish_branch:mainpublish_dir:./publiccname:chengleqi.com这个脚本使用了peaceiris/actions-hugo@v2和peaceiris/actions-gh-pages@v3两个action分别用于设置hugo环境以及推送至gh-pages。 技巧 存储blog源文件的repo访问github.io repo需要设置personal_token: ${{ secrets.ACTION_ACCESS_TOKEN }} 在Deploy job中需要指定CNAME，会自动生成CNAME文件，当然也可以手动添加，参考Hugo官网。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:2","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"启用algolia搜索 因为algolia需要上传索引文件，主题的作者推荐了一款Algolia Atomic插件用来完成自动化上传。 这款插件需要node和npm，我参考了这篇文章，直接在GitHub Actions分配的VPS中配置不成功。 于是找到了这个方案，作者将atomic-algolia封装进docker容器中，启动立即执行atomic-aloglia。将docker启动命令附加到pipeline脚本中，再次推送，一次成功，只是多出了拉取镜像的时间。我已将镜像同步至Docker Hub进行解耦。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:3","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"}]