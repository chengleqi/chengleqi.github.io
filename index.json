[{"categories":["Kubernetes"],"content":"使用kind创建集群 安装好kind后直接kind create cluster --config config.yaml --name cluster启动集群，config.yaml如下，是一个有一个master两个node的测试集群。测试完成后kind delete cluster --name cluster即可删除集群。 # three node (two workers) cluster configkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4nodes:- role:control-plane- role:worker- role:worker在尝试过minikube和kind后还是选择了kind。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:1:0","tags":["Prometheus"],"title":"Kubernetes部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"部署kube-prometheus 一般在集群中部署Prometheus有三种方法：Prometheus Operator、kube-prometheus 和 community helm chart。 部署方式比较 Prometheus Operator The Prometheus Operator uses Kubernetes custom resources to simplify the deployment and configuration of Prometheus, Alertmanager, and related monitoring components. kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. This chart is maintained by the Prometheus community. For more information, please see the chart’s readme kube-prometheus提供了许多开箱即用的配置，例如Alertmanager、node-exporter以及Grafana dashboards。 将kube-prometheus项目clone至本地，然后执行: # Create the namespace and CRDs, and then wait for them to be available before creating the remaining resources kubectl create -f manifests/setup until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done kubectl create -f manifests/ 由于创建镜像需要拉取的镜像托管在quay.io，会出现ImagePullBackOff错误： ImagePullBackOff\" ImagePullBackOff k describe pods -n monitoring prometheus-operator-75d9b475d9-f29g8检查一下，是 quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 两个镜像拉取失败，这时借助阿里云容器镜像服务进行拉取。 阿里云容器镜像服务构建镜像步骤 创建GitHub repo，路径格式为：image-name/tag/Dockerfile 在阿里云容器镜像服务中创建仓库image-name 选择代码源为GitHub repo，选择海外构建 进入仓库，添加构建规则，立即构建 构建完成后登录到阿里云容器镜像服务： docker login --username=xxx registry.cn-hangzhou.aliyuncs.com 拉取构建好的镜像：docker pull registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 进行retag：docker tag registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 删除之前的镜像：docker rmi registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 brancz/kube-rbac-proxy:v0.11.0也重复上述步骤，此时就有了kube-operator所需的镜像。 使用kind load docker-image quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 --name cluster将镜像加载进kind集群，然后prometheus-operator-75d9b475d9-f29g8这个pod就正常运行了。 kube-operator-pod处于Running状态\" kube-operator-pod处于Running状态 若还有镜像报错，重复上述步骤，直到所有pod都处于Running状态，到此kube-prometheus部署成功。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:2:0","tags":["Prometheus"],"title":"Kubernetes部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"测试kube-prometheus ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:0","tags":["Prometheus"],"title":"Kubernetes部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"Prometheus 这里设置service的端口转发，接着就能进入Prometheus的UI尝试PromQL了。 kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 info 一本Prometheus的书 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:1","tags":["Prometheus"],"title":"Kubernetes部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"grafana 同样端口转发 kubectl --namespace monitoring port-forward svc/grafana 3000 grafana login\" grafana login tip 用户名和密码都是admin 之后就可以尝试自带的面板了(例如node-exporter)： grafana node-exporter\" grafana node-exporter Grafana不仅可以对接Prometheus，还可以对接zabbix等很多数据源，是一个非常漂亮和好用的可视化工具。 info 还有一个自称Kubernetes IDE的工具Lens，我尝试了一下，也很好用。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:2","tags":["Prometheus"],"title":"Kubernetes部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":null,"content":"welcome amigo~ Tips 这里并没有什么东西☕ bye~🌒🍜 ","date":"2021-08-03","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Hugo"],"content":"效果 在vscode中写作，写完直接commit push，完成文章的发布。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:1:0","tags":["Docker","CI/CD"],"title":"Docker+GitHub Actions优化博客workflow","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"workflow 如下图: workflow(点击图片跳转至Live Editor) 上图是用mermaid画的，flowchart是mermaid高版本的feature，在我使用Feelit主题shortcode的时候还没有得到支持，提了个issue，在这次更新中得到了支持。 mermaid shortcode在手机浏览器中没有得到很好的支持，换回figure shortcode。(2021年 08月 04日 星期三 15:30:15 CST) ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:0","tags":["Docker","CI/CD"],"title":"Docker+GitHub Actions优化博客workflow","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用Docker进行本地预览 参考了jojomi/docker-hugo重写了Dockerfile和构建镜像并且启动容器的脚本chengleqi/docker-hugo。 镜像的README 在浏览器中下载最新版hugo binary(extend版本) 将下载好的hugo binary放在项目根目录 执行./build_and_start_hugo.sh 构建镜像并运行容器 打开浏览器 localhost:1313 注意 在docker run的时候挂载了blog目录用于hugo server，并添加了--rm参数，退出容器后直接删除容器。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:1","tags":["Docker","CI/CD"],"title":"Docker+GitHub Actions优化博客workflow","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用GitHub Actions自动构建并部署 此处参考了Arnab Kumar Shil的文章。配置了GitHub Actions的流水线。脚本如下: name:github pageson:push:branches:- main # Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-20.04steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'latest'extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.ref == 'refs/heads/main'with:personal_token:${{ secrets.ACTION_ACCESS_TOKEN }}external_repository:chengleqi/chengleqi.github.iopublish_branch:mainpublish_dir:./publiccname:chengleqi.com这个脚本使用了peaceiris/actions-hugo@v2和peaceiris/actions-gh-pages@v3两个action分别用于设置hugo环境以及推送至gh-pages。 技巧 存储blog源文件的repo访问github.io repo需要设置personal_token: ${{ secrets.ACTION_ACCESS_TOKEN }} 在Deploy job中需要指定CNAME，会自动生成CNAME文件，当然也可以手动添加，参考Hugo官网。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:2","tags":["Docker","CI/CD"],"title":"Docker+GitHub Actions优化博客workflow","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"启用algolia搜索 因为algolia需要上传索引文件，主题的作者推荐了一款Algolia Atomic插件用来完成自动化上传。 这款插件需要node和npm，我参考了这篇文章，直接在GitHub Actions分配的VPS中配置不成功。 于是找到了这个方案，作者将atomic-algolia封装进docker容器中，启动立即执行atomic-aloglia。将docker启动命令附加到pipeline脚本中，再次推送，一次成功，只是多出了拉取镜像的时间。我已将镜像同步至Docker Hub进行解耦。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:3","tags":["Docker","CI/CD"],"title":"Docker+GitHub Actions优化博客workflow","uri":"/posts/21-08-01/hugo/"}]