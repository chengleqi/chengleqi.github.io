[{"categories":["Kubernetes"],"content":"Service是虚拟的！ ","date":"2022-02-10","objectID":"/posts/22-02-10/:0:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service是什么 Pod中的容器可以通过pause容器共享网络命名空间实现互访 同一个Node内的不同Pod可以通过Docker0网桥实现互访 不同Node上的Pod可以通过CNI插件，例如Flannel实现互访 通过以上三点，一个覆盖了k8s集群的扁平化Pod互访网络就建立起来了，这个Overlay Network中Pod的IP地址由Flannel进行划分。 但是Pod的生命周期是短暂的，Pod的死亡、Pod重新被拉起、Pod的扩容都会导致IP地址的变化。所以不能在Pod内直接使用IP地址来进行Pod间的访问。我们需要一个持久的静态IP来实现Pod间的访问，同时实现负载均衡和服务发现。Service是一个抽象出来的k8s资源，是由每个worker node上的kube-proxy组件通过修改iptables（iptables mode）来实现的。 可以将Service理解为一个虚拟IP。Service的IP空间又是独立于Pod的IP和Node的IP，可以被人为指定，所以一般来讲k8s的网络空间被分为三层。Pod的IP是由Flannel划分的，一个Node上一个子空间；Node的IP就是真实物理网卡IP；再加上Service的IP，组成三层网络模型。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:1:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"Service如何实现 Service是由kube-proxy组件通过修改iptables实现的。 wikipedia iptables是运行在用户空间的应用软件，通过控制Linux内核netfilter模块，来管理网络数据包的处理和转发。 iptables Process Flow\" iptables Process Flow kube-proxy主要设置了iptables中的nat表，通过在nat表中设置PREROUTING和OUTPUT链来劫持网络数据，负载均衡地导向至Service选择到的Pod。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:2:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"一个例子 具体的yaml就不展示了，清单里有一个Service，Service选择一个Pod，Pod有8080、1985、1935三个端口。 此时使用iptables -t nat -nvL查看nat表中的所有链，发现PREROUTING和OUTPUT链均在头部插入了一个KUBE-SERVICES。这个KUBE-SERVICES链就是k8s的Service的门户，Service就是通过这个链劫持流量，进行DNAT和SNAT完成Pod间的互访。 通过kubectl get svc可以查看到Service的Cluster IP是10.43.69.202。接着可以跟踪KUBE-SERVICES。如下 KUBE-SERVICES链\" KUBE-SERVICES链 可以看出所有目的地为10.43.69.202（也就是Service的Cluster IP）且目的端口为1935的数据包都被导向至KUBE-SVC-FQTBCD7TKAEMWX5M链，可以理解为就是Service。接着跟踪KUBE-SVC-FQTBCD7TKAEMWX5M链。如下 KUBE-SVC-FQTBCD7TKAEMWX5M链\" KUBE-SVC-FQTBCD7TKAEMWX5M链 发现只有一个KUBE-SEP-CQBFNZNKSCBZ27C2链，SEP意思就是Service End Point。因为我的Service下只有一个Pod所以这个地方只有一个SEP链，如果Service选择了多个Pod的话，这里应该有对应的endPoint数量的SEP链数，并且使用iptables statistic 模块做round-robin的负载均衡。其实看到这里就可以发现使用iptables模式实现Service的一些不足，例如：如果创建了许多的Service，那么iptables的链数就会变得肿胀，性能就会下降。而且iptables只有RR这一种负载均衡策略，但现实是我们需要更多的负载均衡策略，于是在Kubernetes1.14版本以后，就默认使用ipvs模式来替代iptables模式。 继续跟进KUBE-SEP-CQBFNZNKSCBZ27C2链可以发现 KUBE-SEP-CQBFNZNKSCBZ27C2链\" KUBE-SEP-CQBFNZNKSCBZ27C2链 最后的DNAT，也就是目的地址转换，将目的地址从最初的虚拟的Service Cluster IP转换至Pod的IP和目的端口1935。 NodePort类型 Service常见的有类型ClusterIP（默认）、NodePort和Load Balancer。 在常见的情况下，集群内Pod间的互访是通过设置Service Cluster IP命中OUTPUT链最终被导向Pod。 而集群外的请求进入集群内则需要将Service设置为NodePort（生产环境中不这么做），kube-proxy会在每个worker node上开启端口，外部请求会命中PREROUTING链最后的KUBE-NODEPORTS链，匹配规则是\"ADDRTYPE match dst-type LOCAL\"。之后会继续根据端口导向至对应的Service链。 ","date":"2022-02-10","objectID":"/posts/22-02-10/:3:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["Kubernetes"],"content":"总结 k8s中的Service是一个抽象出来的功能概念。 我们可以将Service想象成为一个接口，它定义了几个功能： 有贯穿Service生命周期的固定的IP和端口 服务发现 负载均衡 而iptables模式是对Service的一个具体实现： kube-proxy通过设置iptables对流量进行转发 kube-proxy通过watch api-server（是否有Pod死亡？重新拉起？扩容？）来改写iptables规则 在iptables的SVC链中对SEP链进行RR的负载均衡 iptables实现Service可以用一张图总结: 通过iptables实现Service\" 通过iptables实现Service 所以我们可以说Service是通过kube-proxy和iptables联合实现的，是为Pod创造出来的假象。 参考： https://www.jianshu.com/p/67744d680286 https://www.cnblogs.com/charlieroro/p/9588019.html ","date":"2022-02-10","objectID":"/posts/22-02-10/:4:0","tags":["Network"],"title":"k8s Service简记","uri":"/posts/22-02-10/"},{"categories":["IaC"],"content":"可能是要鸽了？ IaC(Infrastructure as code)，旨在提升基础设施管理的便捷性，基础设施抽象成code之后更加灵活，便于进行版本控制，也方便了CI\\CD。 ","date":"2021-10-05","objectID":"/posts/21-10-05/:0:0","tags":["Terraform","Pulumi"],"title":"IaC: 从Terraform到Pulumi","uri":"/posts/21-10-05/"},{"categories":["Kubernetes"],"content":"听说kube-prometheus开箱即用？ ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:0:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"使用kind创建集群 安装好kind后直接kind create cluster --config config.yaml --name cluster启动集群，config.yaml如下，是一个有一个master两个node的测试集群。测试完成后kind delete cluster --name cluster即可删除集群。 # three node (two workers) cluster configkind:ClusterapiVersion:kind.x-k8s.io/v1alpha4nodes:- role:control-plane- role:worker- role:worker在尝试过minikube和kind后还是选择了kind。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:1:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"部署kube-prometheus 一般在集群中部署Prometheus有三种方法：Prometheus Operator、kube-prometheus 和 community helm chart。 部署方式比较 Prometheus Operator The Prometheus Operator uses Kubernetes custom resources to simplify the deployment and configuration of Prometheus, Alertmanager, and related monitoring components. kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. This chart is maintained by the Prometheus community. For more information, please see the chart’s readme kube-prometheus提供了许多开箱即用的配置，例如Alertmanager、node-exporter以及Grafana dashboards。 将kube-prometheus项目clone至本地，然后执行: # Create the namespace and CRDs, and then wait for them to be available before creating the remaining resources kubectl create -f manifests/setup until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done kubectl create -f manifests/ 由于创建镜像需要拉取的镜像托管在quay.io，会出现ImagePullBackOff错误： ImagePullBackOff\" ImagePullBackOff k describe pods -n monitoring prometheus-operator-75d9b475d9-f29g8检查一下，是 quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 两个镜像拉取失败，这时借助阿里云容器镜像服务进行拉取。 阿里云容器镜像服务构建镜像步骤 创建GitHub repo，路径格式为：image-name/tag/Dockerfile 在阿里云容器镜像服务中创建仓库image-name 选择代码源为GitHub repo，选择海外构建 进入仓库，添加构建规则，立即构建 构建完成后登录到阿里云容器镜像服务： docker login --username=xxx registry.cn-hangzhou.aliyuncs.com 拉取构建好的镜像：docker pull registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 进行retag：docker tag registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 删除之前的镜像：docker rmi registry.cn-hangzhou.aliyuncs.com/chengleqi/prometheus-operator:v0.49.0 brancz/kube-rbac-proxy:v0.11.0也重复上述步骤，此时就有了kube-operator所需的镜像。 使用kind load docker-image quay.io/brancz/kube-rbac-proxy:v0.11.0 quay.io/prometheus-operator/prometheus-operator:v0.49.0 --name cluster将镜像加载进kind集群，然后prometheus-operator-75d9b475d9-f29g8这个pod就正常运行了。 kube-operator-pod处于Running状态\" kube-operator-pod处于Running状态 若还有镜像报错，重复上述步骤，直到所有pod都处于Running状态，到此kube-prometheus部署成功。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:2:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"测试kube-prometheus ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:0","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"Prometheus 这里设置service的端口转发，接着就能进入Prometheus的UI尝试PromQL了。 kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 info 一本Prometheus的书 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:1","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":["Kubernetes"],"content":"grafana 同样设置端口转发 kubectl --namespace monitoring port-forward svc/grafana 3000 tip 用户名和密码都是admin 之后就可以尝试自带的面板了(例如node-exporter)： grafana node-exporter\" grafana node-exporter Grafana不仅可以对接Prometheus，还可以对接zabbix等很多数据源，是一个非常漂亮和好用的可视化工具。 info 还有一个自称Kubernetes IDE的工具Lens，我尝试了一下，也很好用。 ","date":"2021-08-05","objectID":"/posts/21-08-05/kube-prometheus/:3:2","tags":["Prometheus"],"title":"k8s部署kube-prometheus","uri":"/posts/21-08-05/kube-prometheus/"},{"categories":null,"content":" NAME READY STATUS RESTARTS ROLES this is my etcd. 1/1 Running 0 control-panel ","date":"2021-08-03","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Hugo"],"content":"博客还是简洁一点比较好？ ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:0:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"效果 git commit + push即可。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:1:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"workflow 如下图: workflow(点击图片跳转至Live Editor) 上图是用mermaid画的，flowchart是mermaid高版本的feature，在我使用Feelit主题shortcode的时候还没有得到支持，提了个issue，在这次更新中得到了支持。 mermaid shortcode在手机浏览器中没有得到很好的支持，换回figure shortcode。(2021年 08月 04日 星期三 15:30:15 CST) ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:0","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用Docker进行本地预览 参考了jojomi/docker-hugo重写了Dockerfile和构建镜像并且启动容器的脚本chengleqi/docker-hugo。 镜像的README 在浏览器中下载最新版hugo binary(extend版本) 将下载好的hugo binary放在项目根目录 执行./build_and_start_hugo.sh 构建镜像并运行容器 打开浏览器 localhost:1313 注意 在docker run的时候挂载了blog目录用于hugo server，并添加了--rm参数，退出容器后直接删除容器。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:1","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"使用GitHub Actions自动构建并部署 此处参考了Arnab Kumar Shil的文章。配置了GitHub Actions的流水线。脚本如下: name:github pageson:push:branches:- main # Set a branch to deploypull_request:jobs:deploy:runs-on:ubuntu-20.04steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'latest'extended:true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3if:github.ref == 'refs/heads/main'with:personal_token:${{ secrets.ACTION_ACCESS_TOKEN }}external_repository:chengleqi/chengleqi.github.iopublish_branch:mainpublish_dir:./publiccname:chengleqi.com这个脚本使用了peaceiris/actions-hugo@v2和peaceiris/actions-gh-pages@v3两个action分别用于设置hugo环境以及推送至gh-pages。 技巧 存储blog源文件的repo访问github.io repo需要设置personal_token: ${{ secrets.ACTION_ACCESS_TOKEN }} 在Deploy job中需要指定CNAME，会自动生成CNAME文件，当然也可以手动添加，参考Hugo官网。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:2","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"},{"categories":["Hugo"],"content":"启用algolia搜索 因为algolia需要上传索引文件，主题的作者推荐了一款Algolia Atomic插件用来完成自动化上传。 这款插件需要node和npm，我参考了这篇文章，直接在GitHub Actions分配的VPS中配置不成功。 于是找到了这个方案，作者将atomic-algolia封装进docker容器中，启动立即执行atomic-aloglia。将docker启动命令附加到pipeline脚本中，再次推送，一次成功，只是多出了拉取镜像的时间。我已将镜像同步至Docker Hub进行解耦。 ","date":"2021-08-01","objectID":"/posts/21-08-01/hugo/:2:3","tags":["Docker","CI/CD"],"title":"博客搭建简记","uri":"/posts/21-08-01/hugo/"}]